=============
Code overview
=============

User interface
--------------

`Installing <installation.txt>`_ the :code:`bilby_pipe` module provides the
user with four command line programs:

1. `bilby_pipe <main.txt>`_
2. `bilby_pipe_generation <data_generation.txt>`_
3. `bilby_pipe_analysis <data_analysis.txt>`_
4. `bilby_pipe_create_injection_file <create_injections.txt>`_

For most users, only the first of these will be used in practise. In this
document, we'll give a brief overview of how these are used internally to
help developers orient themselves with the project.

Python Modules
--------------

At the top-level, the :code:`bilby_pipe` python package provides several
sub-modules as visualised here:

.. graphviz::

   digraph {
         "bilby_pipe" -> ".main";
         "bilby_pipe" -> ".data_generation";
         "bilby_pipe" -> ".data_analysis";
         "bilby_pipe" -> ".create_injections";
         "bilby_pipe" -> ".utils";
            }

each submodule (e.g., :code:`bilby_pipe.utils`) serves a different purpose.
On this page, we'll give a short description of the general code structure.
Specific details for different modules can then be found by following the
links in the `Submodules API`_.

Workflow
--------

The typical workflow for `bilby_pipe` is that a user calls the
:code:`bilby_pipe` command line tool giving it some "User input". Typically,
this is of the form of an `ini file <ini_file.txt>`_, and any extra command
line arguments. This user input is handled by the `bilby_pipe.main <main.txt>`_
module (which provides the command-line interface). It generates two types of
output, "DAG files" and a "summary webpage" (if requested). I.e., the top-level
workflow looks like this:

.. graphviz::

   digraph {
         rankdir="LR";
         "User input" -> "bilby_pipe.main";
         "bilby_pipe.main" -> "DAG files";
         "bilby_pipe.main" -> "summary webpage";
            }

Depending on the exact type of the job, the DAG may contain a number of jobs.
Typically, there are *generation* and *analysis* jobs. For a simple job, e.g.,
analysing a GraceDB candidate. There may be one generation job (to load the
JSON data from GraceDB, find relevant frame files and make PSDs) and one
analysis job (to run some sampler given some prior etc.). For cases with
multiple components (e.g., create an analysis n injections) things may be
more complicated. The logic for handling all of this is contained within the
`main <main.txt>`_ module.

In the most general case, there will be n parallell jobs with no inter-job
dependencies. Within each of these jobs, there is typically a structure in
the DAG as follows:

.. graphviz::

   digraph {
         rankdir="TD";
         "Data Generation" -> "Data Analysis 1";
         "Data Generation" -> "Data Analysis 2";
         "Data Generation" -> "...";
         "Data Generation" -> "Data Analysis M";
         "Data Analysis 1" -> "post-processing";
         "Data Analysis 2" -> "post-processing";
         "..." -> "post-processing";
         "Data Analysis N" -> "post-processing";
            }

Each Data Analysis job refers to a different way to analyse the same data. For
example, using different samplers, or different subsets of detectors. If there
are M Data Anlaysis jobs and N top-level jobs, there is MN jobs in total.

The "Data Generation" job uses the `bilby_pipe_generation
<data_generation.txt>`_ executable to create all the data which may be
analysed.

The "Data Analysis" jobs uses the `bilby_pipe_analysis
<data_analysis.txt>`_ executable to create all the data which may be
analysed.

See the table of contents below for an overview of the API.

Summary webpage
---------------

:code:`bilby_pipe` allows the user to visualise the posterior samples through
a 'summary' webpage. This is implemented usingÂ `PESummary
<https://git.ligo.org/charlie.hoy/pesummary>`_ (documentation `here
<https://docs.ligo.org/charlie.hoy/pesummary/>`_). This is currently an optional
requirement and must be installed prior to use:

.. code-block:: console

   $ pip install -r optional_requirements.txt

To generate a summary webpage, the :code:`create-summary` option must be passed
in the configuration file as well as the web directory where you would like
the output from :code:`PESummary` to be stored. If you are working on an LDG
cluster, then the web directory should be in your public_html. Below is an
example of a configuration file that will generate 'summary' webpages:

.. code-block:: text

    gracedb = G184098
    label = GW150914
    include-detectors = [H1, L1]
    coherence-test = True
    duration = 4
    outdir = GW150914
    sampler = dynesty
    sampler-kwargs = {'nlive': 500}
    channel_names = [H1:DCS-CALIB_STRAIN_C02, L1:DCS-CALIB_STRAIN_C02]
    prior_file = GW150914.prior

    create-summary = True
    email = albert.einstein
    webdir = /home/albert.einstein/public_html/project

    executable = bbh_from_gracedb.py
    accounting = ligo.dev.o3.cbc.pe.lalinference

If you have already generated a webpage in the past using :code:`PESummary`,
then you are able to pass the :code:`add-to-existing` and :code:`existing-dir`
options to add further results files to a single webpage. This includes all
histograms for each results file as well as comparison plots. Below is an example
configuration file that will add to an existing webpage:

.. code-block:: text

    gracedb = G184098
    label = GW150914
    include-detectors = [H1, L1]
    coherence-test = True
    duration = 4
    outdir = GW150914
    sampler = dynesty
    sampler-kwargs = {'nlive': 500}
    channel_names = [H1:DCS-CALIB_STRAIN_C02, L1:DCS-CALIB_STRAIN_C02]
    prior_file = GW150914.prior

    create-summary = True
    email = albert.einstein
    existing-dir = /home/albert.einstein/public_html/project
    add-to-existing = True

    executable = bbh_from_gracedb.py
    accounting = ligo.dev.o3.cbc.pe.lalinference


Submodules API
--------------
.. toctree::
   :maxdepth: 2

   main
   data_generation
   data_analysis
   create_injections
   utils
